# OpenTranscribe Development Overrides
# This file is AUTOMATICALLY loaded in development (no -f flag needed!)
#
# Usage: Just run `docker compose up` - this file merges with docker-compose.yml
#
# This file contains ONLY development-specific settings:
#   - Build from local Dockerfiles (instead of pulling images)
#   - Mount source code for live reloading
#   - Development-specific commands

services:
  backend:
    image: opentranscribe-backend-dev:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development (no AI models needed)
    command: uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload  # Enable hot reload

  celery-worker:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch
      - ${MODEL_CACHE_DIR:-./models}/nltk_data:/home/appuser/.cache/nltk_data
      - ${MODEL_CACHE_DIR:-./models}/sentence-transformers:/home/appuser/.cache/sentence-transformers

  celery-download-worker:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch

  celery-cpu-worker:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development

  celery-nlp-worker:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development

  celery-beat:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev  # Use development Dockerfile
    volumes:
      - ./frontend:/app  # Mount source code for development
      - /app/node_modules  # Exclude node_modules from host mount
    ports:
      - "${FRONTEND_PORT:-5173}:5173"  # Dev Vite server port
    environment:
      - NODE_ENV=development  # Development mode
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:5173"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  flower:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development
      - flower_data:/app

  # GPU Scaled Worker - Development overrides
  # Base definition in docker-compose.yml, GPU config in docker-compose.gpu-scale.yml
  celery-worker-gpu-scaled:
    image: opentranscribe-backend-dev:latest  # Reuse same image - saves overlay2 space
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    volumes:
      - ./backend:/app  # Mount source code for development
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch
      - ${MODEL_CACHE_DIR:-./models}/nltk_data:/home/appuser/.cache/nltk_data
      - ${MODEL_CACHE_DIR:-./models}/sentence-transformers:/home/appuser/.cache/sentence-transformers

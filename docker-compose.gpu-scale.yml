# docker-compose.gpu-scale.yml
# Optional overlay for multi-GPU worker scaling
#
# This overlay provides full service definition for celery-worker-gpu-scaled,
# enabling parallel GPU workers on a dedicated GPU device for significantly
# increased transcription throughput on systems with multiple GPUs.
#
# Usage:
#   ./opentr.sh start dev --gpu-scale
#   OR
#   docker compose -f docker-compose.yml -f docker-compose.override.yml -f docker-compose.gpu-scale.yml up
#
# Note: This file contains the complete service definition, including build,
# volumes, and GPU configuration. The service is not defined in base docker-compose.yml
# to ensure compatibility with macOS and CPU-only systems.
#
# Configuration:
#   Set these variables in your .env file:
#   - GPU_SCALE_ENABLED=true          # Enable GPU scaling (default: false)
#   - GPU_SCALE_DEVICE_ID=2           # GPU device ID to use (default: 2)
#   - GPU_SCALE_WORKERS=4             # Number of parallel workers (default: 4)
#
# Example Hardware Setup:
#   GPU 0: NVIDIA RTX A6000 (49GB)    - Running LLM model
#   GPU 1: RTX 3080 Ti (12GB)         - Default single worker (disabled when scaling enabled)
#   GPU 2: NVIDIA RTX A6000 (49GB)    - Scaled workers (4 parallel in single container)

services:
  # Disable the default single GPU worker when scaling is enabled
  celery-worker:
    scale: 0

  # Scaled GPU Worker - Single container with multiple concurrent workers
  # This overlay provides full service definition for GPU-scaled worker
  # Image, build, and pull_policy settings come from overlay files (prod.yml, offline.yml)
  celery-worker-gpu-scaled:
    container_name: ${COMPOSE_PROJECT_NAME:-opentranscribe}-celery-worker-gpu-scaled
    scale: 1  # Enable this service
    restart: always
    env_file: .env
    build:
      context: ./backend
      dockerfile: Dockerfile.multiplatform
    volumes:
      # Model cache directories - must be owned by UID 1000 on host
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch
      - ${MODEL_CACHE_DIR:-./models}/nltk_data:/home/appuser/.cache/nltk_data
      - ${MODEL_CACHE_DIR:-./models}/sentence-transformers:/home/appuser/.cache/sentence-transformers
    command: >
      celery -A app.core.celery worker
      -Q gpu
      --concurrency=${GPU_SCALE_WORKERS:-4}
      --prefetch-multiplier=${GPU_SCALE_WORKERS:-4}
      --max-tasks-per-child=10
      -n gpu-scaled@%h
      --loglevel=info
    environment:
      # Internal Docker network settings (same as other workers)
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - MINIO_HOST=minio
      - MINIO_PORT=9000
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OPENSEARCH_HOST=opensearch
      - OPENSEARCH_PORT=9200
      - MODELS_DIRECTORY=/app/models
      - MODEL_BASE_DIR=/app/models
      - TEMP_DIR=/app/temp
      - CUDA_VISIBLE_DEVICES=${GPU_SCALE_DEVICE_ID:-2}
    healthcheck:
      test: ["CMD-SHELL", "celery -A app.core.celery inspect ping -d gpu-scaled@$$HOSTNAME"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${GPU_SCALE_DEVICE_ID:-2}"]
              capabilities: [gpu]
    depends_on:
      - postgres
      - redis
      - minio
      - opensearch
